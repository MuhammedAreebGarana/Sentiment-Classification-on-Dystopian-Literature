# -*- coding: utf-8 -*-
"""SentimentClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/146n-GXEBp3ogMWJY7lsavXzxKpY0GR_f
"""

from google.colab import files
uploaded = files.upload()

!apt-get install unrar

import os

# Define the path to the uploaded RAR file
rar_file_path = 'dataVizFiles.rar'

# Create a directory to extract the files
extraction_path = 'dataVizFiles'
os.makedirs(extraction_path, exist_ok=True)

# Extract the RAR file
!unrar x {rar_file_path} {extraction_path}/

# List the files in the extracted directory
extracted_files = os.listdir(extraction_path)
print(extracted_files)

!pip install textblob
!pip install matplotlib
!pip install seaborn

import os
import pandas as pd

# Define the path to the extracted folder
extracted_folder = 'dataVizFiles/dataVizFiles'

# Create a list to hold the text data
texts = []

# Loop through each file in the directory
for filename in os.listdir(extracted_folder):
    if filename.endswith('.txt'):  # Ensure we only read .txt files
        file_path = os.path.join(extracted_folder, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            texts.append(file.read())

# Create a DataFrame from the text data
df = pd.DataFrame({'text': texts, 'filename': os.listdir(extracted_folder)})
print(df.head())  # Display the first few rows of the DataFrame

import re
import string
import nltk

# Download necessary NLTK packages
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')

from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords  # Corrected import statement

# Function for text preprocessing
def preprocess_text(text):
    # Remove special characters and punctuation
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Assuming df is your DataFrame and it has a 'text' column
df['cleaned_text'] = df['text'].apply(preprocess_text)
print(df[['filename', 'text', 'cleaned_text']].head())  # Display the first few rows

from textblob import TextBlob

def classify_emotion(text):
    analysis = TextBlob(text)
    # Classify sentiment
    if analysis.sentiment.polarity > 0:
        return 'Positive'
    elif analysis.sentiment.polarity == 0:
        return 'Neutral'
    else:
        return 'Negative'

df['emotion'] = df['text'].apply(classify_emotion)

from textblob import TextBlob

# Function to get sentiment
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity  # Returns a value between -1 (negative) and 1 (positive)

# Apply sentiment analysis
df['sentiment'] = df['cleaned_text'].apply(get_sentiment)
print(df[['filename', 'cleaned_text', 'sentiment']].head())  # Display the results

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style
sns.set(style="whitegrid")

# Create a bar plot for sentiment scores
plt.figure(figsize=(12, 6))
sns.barplot(x='filename', y='sentiment', data=df)
plt.xticks(rotation=45, ha='right')
plt.title('Sentiment Scores of Texts')
plt.xlabel('Text File')
plt.ylabel('Sentiment Score')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Vectorization
vectorizer = TfidfVectorizer(stop_words='english')
tfidf = vectorizer.fit_transform(df['text'])

# LDA Model
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(tfidf)

# Display topics
for index, topic in enumerate(lda.components_):
    print(f'Topic {index}:')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])

from collections import Counter

def character_frequency(text):
    return Counter(text)

df['char_freq'] = df['text'].apply(character_frequency)

df['emotion'] = df['text'].apply(classify_emotion)
df['char_freq'] = df['text'].apply(character_frequency)

# For thematic analysis, you may want to run it separately and merge results

from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
import nltk
nltk.download('vader_lexicon')


# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment scores
def get_sentiment_score(text):
    return sia.polarity_scores(text)

# Apply sentiment analysis
df['sentiment_scores'] = df['cleaned_text'].apply(get_sentiment_score)

# Extract compound score for overall sentiment
df['compound'] = df['sentiment_scores'].apply(lambda x: x['compound'])

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from sklearn.model_selection import train_test_split

# Tokenization and encoding
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
encodings = tokenizer(df['cleaned_text'].tolist(), truncation=True, padding=True, return_tensors='pt')

# Create a dataset class
class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings['input_ids'])

# Create dataset
dataset = SentimentDataset(encodings)

# Load model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary sentiment

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()

def categorize_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['sentiment'] = df['compound'].apply(categorize_sentiment)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='sentiment')
plt.title('Sentiment Distribution')
plt.show()

print(df.columns)
print(df.head())

# Extract author names from the filename
df['author'] = df['filename'].apply(lambda x: x.split('___')[0])

# Check the updated DataFrame
print(df[['filename', 'author']].head())

# Group by author and calculate the average compound sentiment score
author_sentiment = df.groupby('author')['compound'].mean().reset_index()

# Visualize the average sentiment by author
import seaborn as sns
import matplotlib.pyplot as plt

sns.barplot(data=author_sentiment, x='author', y='compound')
plt.title('Average Sentiment by Author')
plt.xticks(rotation=45)
plt.show()

from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd

# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to classify sentiment as positive or negative
def classify_sentiment(text):
    score = sia.polarity_scores(text)['compound']
    return 'positive' if score >= 0 else 'negative'

# Apply binary sentiment classification
df['binary_sentiment'] = df['cleaned_text'].apply(classify_sentiment)

# Check the results
print(df[['cleaned_text', 'binary_sentiment']].head())

def classify_emotion(text):
    # Truncate text if it exceeds the maximum length
    max_length = 512
    if len(text) > max_length:
        text = text[:max_length]
    return emotion_pipeline(text)

# Install NLTK if not already installed
!pip install nltk

import os
import nltk
import re
from nltk.corpus import stopwords
from nltk import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Define stop words
stop_words = set(stopwords.words('english'))

# Define the path to the extracted folder
extracted_folder = 'dataVizFiles/dataVizFiles'

# Load text files from the directory
texts = []
for file_name in os.listdir(extracted_folder):
    if file_name.endswith('.txt'):
        with open(os.path.join(extracted_folder, file_name), 'r', encoding='utf-8') as file:
            texts.append(file.read())

def text_preprocessing(text):
    text = re.sub(r'(?<=[.,])(?=[^\s])', r' ', text)  # add white space after '.' or ','
    text = re.sub(r'\\', '', text)  # Replace '\' with ''
    text = re.sub(r'\_', ' ', text)  # Replace '_' with ' '
    text = re.sub(r'\*', '', text)  # Replace '*' with ' '
    text = re.sub(r"\'ll", " will", text)  # change 'll to will
    text = re.sub(r"\'t", " not", text)  # change 't to not
    return text

def mass_text_preprocessing(inlist):
    cleaned_books = []
    for i in range(len(inlist)):
        cleaned_books.append(text_preprocessing(inlist[i]))
    return cleaned_books

# Preprocess the text
cleaned_books = mass_text_preprocessing(texts)

def sent_tokenizer(instr):
    return sent_tokenize(instr)

def mass_sent_tokenizer(inlist):
    out = []
    for n in range(len(inlist)):
        book_n = sent_tokenizer(inlist[n])
        out.append(book_n)
    return out

def word_tokenizer(intext):
    word_tokens = [word_tokenize(t) for t in intext]
    return [word for sublist in word_tokens for word in sublist]  # Flatten the list

# Tokenize sentences
tokenized_sentences = mass_sent_tokenizer(cleaned_books)

# Tokenize words
word_tokens = [word_tokenizer(sentences) for sentences in tokenized_sentences]

# Print cleaned texts
for i, cleaned in enumerate(cleaned_books):
    print(f"Cleaned Text for Book {i+1}:\n{cleaned}\n")

# Print tokenized sentences
for i, sentences in enumerate(tokenized_sentences):
    print(f"Tokenized Sentences for Book {i+1}:\n{sentences}\n")

# Print word tokens
for i, tokens in enumerate(word_tokens):
    print(f"Word Tokens for Book {i+1}:\n{tokens}\n")

def character_frequency(text):
    """Calculate the frequency of each character in the text."""
    freq = {}
    for char in text:
        if char.isalpha() or char.isspace():  # Count only alphabetic characters and spaces
            freq[char] = freq.get(char, 0) + 1
    return freq

# Calculate character frequency for each cleaned book
char_frequencies = []
for cleaned_book in cleaned_books:
    freq = character_frequency(cleaned_book)
    char_frequencies.append(freq)

# Print character frequencies for each book
for i, freq in enumerate(char_frequencies):
    print(f"Character Frequency for Book {i+1}:")
    for char, count in sorted(freq.items()):
        print(f"'{char}': {count}")
    print("\n")  # Add a newline for better readability

from transformers import pipeline

# Load the pre-trained emotion classification model
emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")

import os
import re
from transformers import pipeline
from nltk.tokenize import sent_tokenize
import nltk

# Download the punkt tokenizer
nltk.download('punkt')

# Load the pre-trained emotion classification model
emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")

# Define the path to the extracted folder
extracted_folder = 'dataVizFiles/dataVizFiles'

# Load text files from the directory
texts = []
for file_name in os.listdir(extracted_folder):
    if file_name.endswith('.txt'):
        with open(os.path.join(extracted_folder, file_name), 'r', encoding='utf-8') as file:
            texts.append(file.read())

def text_preprocessing(text):
    text = re.sub(r'(?<=[.,])(?=[^\s])', r' ', text)  # add white space after '.' or ','
    text = re.sub(r'\\', '', text)  # Replace '\' with ''
    text = re.sub(r'\_', ' ', text)  # Replace '_' with ' '
    text = re.sub(r'\*', '', text)  # Replace '*' with ' '
    text = re.sub(r"\'ll", " will", text)  # change 'll to will
    text = re.sub(r"\'t", " not", text)  # change 't to not
    return text

def mass_text_preprocessing(inlist):
    cleaned_books = []
    for i in range(len(inlist)):
        cleaned_books.append(text_preprocessing(inlist[i]))
    return cleaned_books

# Preprocess the text
cleaned_books = mass_text_preprocessing(texts)

# Tokenize sentences
tokenized_sentences = [sent_tokenize(book) for book in cleaned_books]

# Flatten the list of tokenized sentences for emotion classification
flattened_sentences = [sentence for sublist in tokenized_sentences for sentence in sublist]

# Use the emotion classifier on the cleaned and tokenized sentences
for cleaned_text in flattened_sentences:
    results = emotion_classifier(cleaned_text)
    print(f"Text: {cleaned_text}")
    for result in results:
        print(f"  Emotion: {result['label']}, Score: {result['score']:.4f}")

# Assuming cleaned_books is your preprocessed list of texts
emotion_results = []

for cleaned_text in cleaned_books:
    results = emotion_classifier(cleaned_text)
    emotion_results.append(results)

import pandas as pd
from collections import defaultdict

# Initialize a dictionary to hold aggregated scores
aggregated_scores = defaultdict(float)

# Aggregate scores
for result in emotion_results:
    for emotion in result:
        aggregated_scores[emotion['label']] += emotion['score']

# Convert to DataFrame for easier manipulation
emotion_df = pd.DataFrame(aggregated_scores.items(), columns=['Emotion', 'Score'])
emotion_df = emotion_df.sort_values(by='Score', ascending=False)

# Display the aggregated scores
print(emotion_df)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.bar(emotion_df['Emotion'], emotion_df['Score'], color='skyblue')
plt.title('Aggregated Emotion Scores')
plt.xlabel('Emotions')
plt.ylabel('Scores')
plt.xticks(rotation=45)
plt.show()